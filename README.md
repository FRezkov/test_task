1. Потраченное время 5 часов. 
Datavault был выбран в качестве модели хранения данных т.к. модель хорошо масшабируется при добавлении новых сущностей.
Также модель очень гибкая и ее легче поддерживать.
Все подробности в скриптах: task_1_1_source.sql, task_1_2_build_target_model.sql, task_1_3_analysis.sql.
2. Потраченное время 24 часа.
Реконсиляция данных была осуществлена в с помощью параллельных процессов работающих в Postgres.
Параметры скрипта лежат в файле rec_variables.json. Максимальное количество параллельных потоков можно указать как max_workers, 
там же можно указать tolerance и другие параметры. Параметры подключения к базе находятся в файле database.ini.
Скрипт task_2_reconsilation.py.
3. Потраченное время 2 часа. 
Агрегаты собираются в 2 параллельных потока по количеству типов агрегаций. Параметры скрипта лежат в файле agg_variables.json.
Агрегаты собираются за определенный промежуток времени определяемый техническим временем заполнения источника данных.
Окно времени можно задать в параметре load_period. Скрипт можно запускать по расписанию. Интервал запуска скрипта должен быть меньше чем 
load_period. Скрипт task_3_aggregate.py.
